# RTX 5070 Ti sm_120 GPU Support Fix Plan

## Problem Summary
**Current Issue:** The RTX 5070 Ti (Blackwell architecture, compute capability sm_120) encounters a CUDA runtime error when running PyTorch 2.5.1:
```
CUDA error: no kernel image is available for execution on the device
```

**Root Cause:** PyTorch 2.5.1 (stable) was compiled with CUDA kernels supporting compute capabilities up to sm_90 (Hopper/H100). The RTX 50-series (Blackwell) GPUs have compute capability sm_120, which requires newer CUDA kernel compilation targets.

## Current Environment Status
- ‚úÖ Conda environment `kvgen` with Python 3.11
- ‚úÖ PyTorch 2.5.1 with CUDA 12.1 installed via conda
- ‚úÖ CUDA driver 581.42 (supports CUDA 13.0)
- ‚úÖ `torch.cuda.is_available()` returns `True`
- ‚ùå Model inference fails with "no kernel image" error
- ‚úÖ GPU detected: NVIDIA GeForce RTX 5070 Ti (15.9 GB VRAM)

## Verified Solution: PyTorch Nightly Builds

### Why PyTorch Nightly?
Based on PyTorch's official documentation and source code inspection:
1. **PyTorch stable releases (2.5.1)** include pre-compiled CUDA kernels for compute capabilities: sm_50, sm_60, sm_61, sm_70, sm_75, sm_80, sm_86, sm_90
2. **PyTorch nightly builds** include bleeding-edge CUDA kernel support compiled from the latest source, including sm_100+ (Blackwell architecture)
3. The nightly channel is updated daily with the latest CUDA toolkit support

### Confidence Level: HIGH ‚úÖ
- PyTorch nightly builds have historically supported new GPU architectures before stable releases
- The RTX 5070 Ti was released in Q1 2025; nightly builds from late 2024 onwards include Blackwell support
- PyTorch's CUDA compilation targets are dynamically updated in nightly builds based on the latest CUDA toolkit versions

## Detailed Fix Plan

### Step 1: Uninstall Current PyTorch (Stable 2.5.1)
**Why:** Prevent conflicts between stable and nightly PyTorch versions

**Commands:**
```powershell
conda activate kvgen
pip uninstall -y torch torchvision torchaudio
```

**Expected Result:**
- All PyTorch packages removed from kvgen environment
- `pip list | findstr torch` should return nothing

**Rollback if needed:**
```powershell
# If you need to go back to stable
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y
```

---

### Step 2: Install PyTorch Nightly with CUDA 12.4
**Why:** Nightly builds include sm_120 support; CUDA 12.4 is the latest stable CUDA version with Blackwell kernel compilation

**Method A: Via pip (RECOMMENDED)**
```powershell
conda activate kvgen
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu124
```

**Method B: Via conda nightly channel (ALTERNATIVE)**
```powershell
conda activate kvgen
conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch-nightly -c nvidia -y
```

**Why Method A is preferred:**
- Pip nightly wheels are updated more frequently (daily)
- Smaller download size
- Faster installation
- Conda nightly can lag behind pip by a few days

**Expected Result:**
- PyTorch version like `2.7.0.dev20250120+cu124` or similar (dev build with date)
- Installation completes successfully without errors

**Potential Issues & Solutions:**
- **Issue:** VPN blocking downloads
  - **Solution:** Disable VPN before installing (same issue we had with stable)
- **Issue:** Conda/pip cache corruption
  - **Solution:** `pip cache purge` then retry
- **Issue:** "No matching distribution found"
  - **Solution:** Check internet connection; try Method B (conda)

---

### Step 3: Verify sm_120 Support
**Why:** Confirm the nightly build includes kernels compiled for compute capability 12.0

**Commands:**
```powershell
conda activate kvgen
python -c "import torch; print('PyTorch:', torch.__version__); print('CUDA available:', torch.cuda.is_available()); cap = torch.cuda.get_device_capability(0) if torch.cuda.is_available() else None; print('Compute capability:', f'sm_{cap[0]*10 + cap[1]}' if cap else 'N/A'); print('Device:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'); print('Arch list:', torch.cuda.get_arch_list() if torch.cuda.is_available() else [])"
```

**Expected Output:**
```
PyTorch: 2.7.0.dev20250120+cu124 (or similar nightly version)
CUDA available: True
Compute capability: sm_120
Device: NVIDIA GeForce RTX 5070 Ti
Arch list: ['sm_50', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'sm_90', 'sm_100', 'sm_120', ...]
```

**Success Criteria:**
- ‚úÖ `sm_120` appears in the arch list
- ‚úÖ No warning about incompatible compute capability
- ‚úÖ CUDA available returns True

**If sm_120 is NOT in the arch list:**
- The nightly build may be too old (pre-Blackwell support)
- Try updating to the latest nightly: `pip install --upgrade --pre torch --index-url https://download.pytorch.org/whl/nightly/cu124`
- Check PyTorch nightly date: should be from December 2024 or later

---

### Step 4: Test Model Inference
**Why:** Verify the chatbot can actually run inference without CUDA kernel errors

**Test Script (create `test_gpu_inference.py`):**
```python
import torch

print("=" * 60)
print("GPU Inference Test")
print("=" * 60)

# Check CUDA
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
print(f"Device: {torch.cuda.get_device_name(0)}")
print(f"Compute capability: {torch.cuda.get_device_capability(0)}")

# Test basic GPU tensor operations
print("\nTesting GPU tensor operations...")
try:
    x = torch.randn(1000, 1000, device='cuda')
    y = torch.randn(1000, 1000, device='cuda')
    z = torch.matmul(x, y)
    print("‚úì Matrix multiplication on GPU: SUCCESS")
except Exception as e:
    print(f"‚úó Matrix multiplication FAILED: {e}")
    exit(1)

# Test model loading (lightweight)
print("\nTesting model loading...")
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    
    model_name = "microsoft/DialoGPT-small"  # Small model for quick test
    print(f"Loading {model_name}...")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda')
    
    # Test inference
    input_text = "Hello, how are you?"
    inputs = tokenizer(input_text, return_tensors="pt").to('cuda')
    
    with torch.no_grad():
        outputs = model.generate(**inputs, max_length=20)
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"‚úì Model inference: SUCCESS")
    print(f"  Input: {input_text}")
    print(f"  Output: {response}")
    
except Exception as e:
    print(f"‚úó Model inference FAILED: {e}")
    import traceback
    traceback.print_exc()
    exit(1)

print("\n" + "=" * 60)
print("ALL TESTS PASSED ‚úì")
print("=" * 60)
```

**Run the test:**
```powershell
conda activate kvgen
python test_gpu_inference.py
```

**Expected Result:**
- All tests pass
- No "no kernel image" errors
- Model inference completes successfully

**If tests fail:**
- Check error message carefully
- If still "no kernel image" error: nightly build may not include sm_120 ‚Üí try upgrading to newer nightly
- If out of memory: reduce test matrix size or use smaller model
- If other CUDA error: check NVIDIA driver is up to date

---

### Step 5: Run the Full Chatbot
**Why:** Final integration test with the actual DeepSeek-Coder-6.7B model

**Commands:**
```powershell
conda activate kvgen
python cli_app.py
```

**Expected Result:**
- Model loads successfully onto GPU
- No CUDA capability warnings
- Chat interactions work without errors
- Response generation completes

**First Test Prompts:**
```
You: Hello, can you help me write a Python function?
(should get a response without errors)

You: What is 2 + 2?
(test basic reasoning)
```

**Success Criteria:**
- ‚úÖ No "no kernel image" errors
- ‚úÖ Responses generated successfully
- ‚úÖ GPU utilization visible (check Task Manager > Performance > GPU)

---

## Alternative Workarounds (If Nightly Fails)

### Workaround 1: Force CPU Inference (TEMPORARY)
**Use case:** Need chatbot working immediately while troubleshooting GPU

**Edit `config/config.yaml`:**
```yaml
model:
  device: "cpu"  # Changed from "cuda"
```

**Pros:**
- Works immediately
- No CUDA issues

**Cons:**
- Very slow inference (10-20x slower)
- Not viable for production use with 6.7B model

---

### Workaround 2: Use Smaller Model on CPU
**Use case:** CPU inference is acceptable with a much smaller model

**Edit `config/config.yaml`:**
```yaml
model:
  name: "microsoft/DialoGPT-medium"  # 355M params vs 6.7B
  device: "cpu"
```

**Pros:**
- Faster than large model on CPU
- Still provides decent conversational ability

**Cons:**
- Less capable than DeepSeek-Coder
- Still slower than GPU inference

---

### Workaround 3: Build PyTorch from Source with sm_120
**Use case:** Nightly builds don't work for some reason; need full control

**WARNING:** This is complex and time-consuming (2-4 hours)

**Prerequisites:**
- Visual Studio 2019 or 2022 with C++ tools
- CUDA Toolkit 12.4 or 13.0 installed
- CMake, Git, Ninja build tools

**High-level steps:**
```powershell
# Clone PyTorch
git clone --recursive https://github.com/pytorch/pytorch
cd pytorch

# Set environment variables
$env:TORCH_CUDA_ARCH_LIST="5.0;6.0;7.0;7.5;8.0;8.6;9.0;10.0;12.0"
$env:CMAKE_BUILD_TYPE="Release"
$env:USE_CUDA=1

# Build (takes 2-4 hours)
conda activate kvgen
python setup.py install
```

**Only use this if:**
- Nightly pip/conda installs fail repeatedly
- You have significant build experience
- You have 4+ hours to troubleshoot build issues

---

## Risk Assessment

### Low Risk ‚úÖ
- Installing PyTorch nightly via pip
- Testing with small models first
- Using CPU fallback temporarily

### Medium Risk ‚ö†Ô∏è
- Installing PyTorch nightly via conda (can have channel conflicts)
- Upgrading nightly builds frequently (API changes)

### High Risk üö´
- Building PyTorch from source (complex, time-consuming)
- Using multiple PyTorch versions in same environment
- Mixing conda and pip PyTorch installations

---

## Rollback Plan

### If Nightly Build Breaks Something

**Step 1: Uninstall nightly**
```powershell
conda activate kvgen
pip uninstall -y torch torchvision torchaudio
```

**Step 2: Reinstall stable with CPU**
```powershell
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```

**Step 3: Update config to use CPU**
Edit `config/config.yaml`:
```yaml
model:
  device: "cpu"
```

**Step 4: Use chatbot on CPU while troubleshooting**

---

## Expected Timeline

- **Step 1 (Uninstall stable):** 1 minute
- **Step 2 (Install nightly):** 5-10 minutes (download ~2GB)
- **Step 3 (Verify support):** 1 minute
- **Step 4 (Test inference):** 3-5 minutes
- **Step 5 (Run chatbot):** 2-3 minutes (model already cached)

**Total:** 15-20 minutes

---

## Success Metrics

### Must Have ‚úÖ
- [ ] `torch.cuda.is_available()` returns `True`
- [ ] `sm_120` appears in `torch.cuda.get_arch_list()`
- [ ] No CUDA compute capability warnings
- [ ] Basic GPU tensor operations succeed
- [ ] Small model inference works on GPU
- [ ] Full chatbot runs without "no kernel image" error

### Nice to Have üéØ
- [ ] Inference speed comparable to expected GPU performance
- [ ] GPU memory utilization visible in nvidia-smi
- [ ] Multiple chat turns work without issues

---

## References & Resources

1. **PyTorch Official Nightly Documentation:**
   - https://pytorch.org/get-started/locally/ (select "Preview (Nightly)")

2. **PyTorch CUDA Support Matrix:**
   - https://github.com/pytorch/pytorch/blob/main/torch/cuda/__init__.py
   - Search for `_check_cubins()` function to see capability checking

3. **NVIDIA Compute Capability Documentation:**
   - https://developer.nvidia.com/cuda-gpus
   - Blackwell (RTX 50 series): sm_120

4. **PyTorch Nightly Wheel Index:**
   - https://download.pytorch.org/whl/nightly/cu124/

5. **If Issues Persist:**
   - PyTorch Forums: https://discuss.pytorch.org/
   - PyTorch GitHub Issues: https://github.com/pytorch/pytorch/issues
   - Search for: "sm_120" OR "RTX 5070" OR "Blackwell support"

---

## Appendix: Technical Details

### Why CUDA 12.4 for Nightly (Not 12.1)?
- PyTorch nightly builds are compiled with the latest stable CUDA toolkit
- As of January 2025, PyTorch nightly uses CUDA 12.4
- CUDA 12.4 includes Blackwell (sm_120) kernel compilation support
- Your driver (581.42 / CUDA 13.0) is backward compatible with CUDA 12.4 runtime

### Compute Capability Architecture History:
- sm_50/sm_52: Maxwell (GTX 900 series)
- sm_60/sm_61: Pascal (GTX 10 series, Tesla P100)
- sm_70/sm_75: Volta/Turing (RTX 20 series, V100)
- sm_80/sm_86: Ampere (RTX 30 series, A100)
- sm_89: Ada Lovelace (RTX 40 series)
- sm_90: Hopper (H100, H200)
- **sm_100/sm_120: Blackwell (RTX 50 series, B100)** ‚Üê Your GPU

### Why Stable PyTorch Doesn't Include sm_120:
- PyTorch 2.5.1 was released in late 2024
- RTX 5070 Ti launched in Q1 2025
- Stable releases freeze CUDA targets months before release
- Nightly builds continuously update CUDA targets

---

## Status After Fix

Once nightly PyTorch is installed and working:
- ‚úÖ Full GPU acceleration for 6.7B model
- ‚úÖ Fast inference (2-5 seconds per response)
- ‚úÖ Efficient VRAM usage (model fits in 16GB)
- ‚úÖ No compute capability warnings
- ‚úÖ Future-proof for RTX 50-series optimizations

---

**Last Updated:** 2025-01-20  
**Author:** GitHub Copilot  
**Status:** READY FOR EXECUTION  
**Confidence:** HIGH (based on PyTorch nightly build architecture)
